{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "detected-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "random.seed(33)\n",
    "\n",
    "from library.evaluation import ConfusionMatrix\n",
    "\n",
    "model_name = \"ReinforcementLearning\"\n",
    "unique_name = \"BERT_Finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "loaded-organic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6425, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.loadtxt(\"../../data/processed/vectors/Phemernr2_BERT_base_finetuned_vectors.txt\", delimiter=\",\")\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "skilled-career",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>label2</th>\n",
       "      <th>topic</th>\n",
       "      <th>tvt</th>\n",
       "      <th>cv_fold</th>\n",
       "      <th>tt</th>\n",
       "      <th>tvt2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>552833795142209536</td>\n",
       "      <td>The East London Mosque would like to offer its...</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>charliehebdo-all-rnr-threads</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>580318210609696769</td>\n",
       "      <td>BREAKING - A Germanwings Airbus A320 plane rep...</td>\n",
       "      <td>rumours</td>\n",
       "      <td>true</td>\n",
       "      <td>germanwings-crash-all-rnr-threads</td>\n",
       "      <td>training</td>\n",
       "      <td>3</td>\n",
       "      <td>training</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552798891994009601</td>\n",
       "      <td>Reports that two of the dead in the #CharlieHe...</td>\n",
       "      <td>rumours</td>\n",
       "      <td>true</td>\n",
       "      <td>charliehebdo-all-rnr-threads</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>576790814942236672</td>\n",
       "      <td>After #Putin disappeared Russian TV no longer ...</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>putinmissing-all-rnr-threads</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>499678822598340608</td>\n",
       "      <td>Saw #Ferguson for myself. #justiceformichaelbr...</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>ferguson-all-rnr-threads</td>\n",
       "      <td>training</td>\n",
       "      <td>3</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                         tweet_text  \\\n",
       "0  552833795142209536  The East London Mosque would like to offer its...   \n",
       "1  580318210609696769  BREAKING - A Germanwings Airbus A320 plane rep...   \n",
       "2  552798891994009601  Reports that two of the dead in the #CharlieHe...   \n",
       "3  576790814942236672  After #Putin disappeared Russian TV no longer ...   \n",
       "4  499678822598340608  Saw #Ferguson for myself. #justiceformichaelbr...   \n",
       "\n",
       "         label       label2                              topic       tvt  \\\n",
       "0  non-rumours  non-rumours       charliehebdo-all-rnr-threads      test   \n",
       "1      rumours         true  germanwings-crash-all-rnr-threads  training   \n",
       "2      rumours         true       charliehebdo-all-rnr-threads      test   \n",
       "3  non-rumours  non-rumours       putinmissing-all-rnr-threads      test   \n",
       "4  non-rumours  non-rumours           ferguson-all-rnr-threads  training   \n",
       "\n",
       "   cv_fold        tt        tvt2  \n",
       "0        2      test    training  \n",
       "1        3  training  validation  \n",
       "2        2      test  validation  \n",
       "3        2      test  validation  \n",
       "4        3  training    training  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../data/processed/phemernr2_dataset_with_tvt.csv\", lineterminator=\"\\n\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9dc307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-rumours', 'true', 'unverified', 'false']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_str = data['label2'].unique().tolist()\n",
    "labels_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f469a1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 0, 0, 0, 2, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for i, d in data.iterrows():\n",
    "    lab = labels_str.index(d['label2'])\n",
    "#     labels.append([1 if j == lab else 0 for j in range(len(labels_str))])\n",
    "    labels.append(lab)\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adverse-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'training'])\n",
    "val_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'validation'])\n",
    "test_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'testting'])\n",
    "\n",
    "train_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'training'])\n",
    "val_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'validation'])\n",
    "test_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'testting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demanding-consortium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4326, 768)\n",
      "(1463, 768)\n",
      "(636, 768)\n",
      "(4326,)\n",
      "(1463,)\n",
      "(636,)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape)\n",
    "print(val_vectors.shape)\n",
    "print(test_vectors.shape)\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3d8296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768]),\n",
      " 2,\n",
      " False,\n",
      " {'target': 0}\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "n_actions = 4\n",
    "\n",
    "class Simulation:\n",
    "    \n",
    "    def __init__(self, vectors, labels):\n",
    "        self.vectors = vectors\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.n_step = 0\n",
    "        self.done = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == self.labels[self.n_step]:\n",
    "            reward = 2\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        self.n_step += 1\n",
    "        if self.n_step >= self.vectors.shape[1]:\n",
    "            self.done = True\n",
    "            \n",
    "        info = {\n",
    "            \"target\": self.labels[self.n_step]\n",
    "        }\n",
    "        return torch.Tensor(self.vectors[self.n_step]), reward, self.done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.n_step = 0\n",
    "        return torch.Tensor(self.vectors[self.n_step])\n",
    "    \n",
    "env = Simulation(train_vectors, train_labels)\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10140856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Linear(n_input, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_output),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94810499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, n_vectors, n_action, save_dir):\n",
    "        self.n_vectors = n_vectors\n",
    "        self.n_action = n_action\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = ClassifierNet(self.n_vectors, self.n_action).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.gamma = 0.7\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def act(self, state, prediction=False):\n",
    "        \"\"\"\n",
    "            Given a state, choose an epsilon-greedy action\n",
    "            \n",
    "            Inputs:\n",
    "            state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "    \n",
    "            Outputs:\n",
    "            action_idx (int): An integer representing which action Mario will perform\n",
    "        \"\"\"\n",
    "        # EXPLORE\n",
    "        if not prediction and np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.n_action)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (LazyFrame),\n",
    "        next_state (LazyFrame),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done(bool))\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "\n",
    "        if self.use_cuda:\n",
    "            state = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action = torch.tensor([action]).cuda()\n",
    "            reward = torch.tensor([reward]).cuda()\n",
    "            done = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action = torch.tensor([action])\n",
    "            reward = torch.tensor([reward])\n",
    "            done = torch.tensor([done])\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca9dc85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime, copy\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab05ab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n",
      "Episode 0 - Step 768 - Epsilon 0.9998080184067972 - Mean Reward 402.0 - Mean Length 768.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.13 - Time 2021-10-20T10:51:13\n",
      "Actions taken : {0, 1, 2, 3}\n",
      "Episode 3000 - Step 3768 - Epsilon 0.9990584434249461 - Mean Reward 0.44 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.98 - Time 2021-10-20T10:51:14\n",
      "Actions taken : {1}\n",
      "Episode 6000 - Step 6768 - Epsilon 0.9983094304136343 - Mean Reward 0.52 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.93 - Time 2021-10-20T10:51:14\n",
      "Actions taken : {2}\n",
      "Episode 9000 - Step 9768 - Epsilon 0.9975609789515425 - Mean Reward 0.6 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.013 - Time 2021-10-20T10:51:15\n",
      "Actions taken : {2}\n",
      "Episode 12000 - Step 12768 - Epsilon 0.9968130886176716 - Mean Reward 0.58 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.162 - Time Delta 4.703 - Time 2021-10-20T10:51:20\n",
      "Actions taken : {3}\n",
      "Episode 15000 - Step 15768 - Epsilon 0.9960657589913281 - Mean Reward 0.62 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.167 - Time Delta 5.032 - Time 2021-10-20T10:51:25\n",
      "Actions taken : {2}\n",
      "Episode 18000 - Step 18768 - Epsilon 0.9953189896521414 - Mean Reward 0.5 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.166 - Time Delta 5.067 - Time 2021-10-20T10:51:30\n",
      "Actions taken : {3}\n",
      "Episode 21000 - Step 21768 - Epsilon 0.9945727801800522 - Mean Reward 0.52 - Mean Length 1.0 - Mean Loss 0.001 - Mean Q Value 0.188 - Time Delta 4.981 - Time 2021-10-20T10:51:35\n",
      "Actions taken : {2}\n",
      "Episode 24000 - Step 24768 - Epsilon 0.9938271301553163 - Mean Reward 0.42 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.188 - Time Delta 5.103 - Time 2021-10-20T10:51:40\n",
      "Actions taken : {3}\n",
      "Episode 27000 - Step 27768 - Epsilon 0.9930820391585105 - Mean Reward 0.58 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.193 - Time Delta 4.923 - Time 2021-10-20T10:51:45\n",
      "Actions taken : {1}\n",
      "Episode 30000 - Step 30768 - Epsilon 0.9923375067705176 - Mean Reward 0.72 - Mean Length 1.0 - Mean Loss 0.001 - Mean Q Value 0.19 - Time Delta 5.072 - Time 2021-10-20T10:51:50\n",
      "Actions taken : {0}\n",
      "Episode 33000 - Step 33768 - Epsilon 0.9915935325725355 - Mean Reward 0.58 - Mean Length 1.0 - Mean Loss 0.001 - Mean Q Value 0.189 - Time Delta 5.218 - Time 2021-10-20T10:51:56\n",
      "Actions taken : {2}\n",
      "Episode 36000 - Step 36768 - Epsilon 0.9908501161460773 - Mean Reward 0.4 - Mean Length 1.0 - Mean Loss 0.001 - Mean Q Value 0.173 - Time Delta 5.141 - Time 2021-10-20T10:52:01\n",
      "Actions taken : {3}\n",
      "Episode 39000 - Step 39768 - Epsilon 0.9901072570729811 - Mean Reward 0.56 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.199 - Time Delta 5.038 - Time 2021-10-20T10:52:06\n",
      "Actions taken : {1}\n",
      "Episode 42000 - Step 42768 - Epsilon 0.9893649549353776 - Mean Reward 0.64 - Mean Length 1.0 - Mean Loss 0.001 - Mean Q Value 0.192 - Time Delta 5.144 - Time 2021-10-20T10:52:11\n",
      "Actions taken : {2}\n",
      "Episode 45000 - Step 45768 - Epsilon 0.9886232093157312 - Mean Reward 0.48 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.193 - Time Delta 5.235 - Time 2021-10-20T10:52:16\n",
      "Actions taken : {2}\n",
      "Episode 48000 - Step 48768 - Epsilon 0.987882019796806 - Mean Reward 0.42 - Mean Length 1.0 - Mean Loss 0.0 - Mean Q Value 0.191 - Time Delta 5.286 - Time 2021-10-20T10:52:21\n",
      "Actions taken : {1}\n",
      "Execution Time : 72.81 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "env = Simulation(train_vectors, train_labels)\n",
    "agent = Classifier(train_vectors.shape[1], n_action=4, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "start = time.time()\n",
    "episodes = 50000\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    acts = []\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = agent.act(state)\n",
    "        acts.append(action)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        agent.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = agent.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 3000 == 0:\n",
    "        logger.record(episode=e, epsilon=agent.exploration_rate, step=agent.curr_step)\n",
    "        print(\"Actions taken :\", set(acts))\n",
    "        \n",
    "print(f\"Execution Time : {round(time.time() - start, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "728882dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Prediction :  0\n",
      "Correct Answer :  0\n"
     ]
    }
   ],
   "source": [
    "idx = 345\n",
    "\n",
    "example = torch.Tensor(val_vectors[idx])\n",
    "prediction = agent.act(example, prediction=True)\n",
    "print(\"Agent Prediction : \", prediction)\n",
    "print(\"Correct Answer : \", val_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "309bba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(agent, vectors):\n",
    "    results = []\n",
    "    for vec in vectors:\n",
    "        results.append(agent.act(torch.Tensor(vec), prediction=True))\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd07cc1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Classification using Reinforcement Learning\n",
      "\n",
      "Validation Set\n",
      "Predictions : (1463,)\n",
      "[0 1 2 3]\n",
      "1463 vs 1463\n",
      "Multi Class Evaluation\n",
      "\n",
      "Class non-rumours Evaluation\n",
      "- Precision : 81.426 %\n",
      "- Recall : 95.49 %\n",
      "- F1 : 0.87899\n",
      "\n",
      "Class true Evaluation\n",
      "- Precision : 82.308 %\n",
      "- Recall : 44.398 %\n",
      "- F1 : 0.57682\n",
      "\n",
      "Class unverified Evaluation\n",
      "- Precision : 71.538 %\n",
      "- Recall : 55.689 %\n",
      "- F1 : 0.62626\n",
      "\n",
      "Class false Evaluation\n",
      "- Precision : 83.942 %\n",
      "- Recall : 78.767 %\n",
      "- F1 : 0.81272\n",
      "\n",
      "Combined Evaluation\n",
      "- Accuracy : 80.861 %\n",
      "- Precision : 79.803 %\n",
      "- Recall : 68.586 %\n",
      "- F1 : 0.73771\n",
      "\n",
      "- Average Confidence : 100.0 %\n",
      "Model, Combined,,,,non-rumours,,,true,,,unverified,,,false,,,\n",
      "ReinforcementLearning Validation, 80.861, 79.803, 68.586, 0.73771, 81.426, 95.49, 0.87899, 82.308, 44.398, 0.57682, 71.538, 55.689, 0.62626, 83.942, 78.767, 0.81272, \n",
      "\n",
      "Test Set\n",
      "Predictions : (636,)\n",
      "[0 1 2 3]\n",
      "636 vs 636\n",
      "Multi Class Evaluation\n",
      "\n",
      "Class non-rumours Evaluation\n",
      "- Precision : 84.096 %\n",
      "- Recall : 95.309 %\n",
      "- F1 : 0.89352\n",
      "\n",
      "Class true Evaluation\n",
      "- Precision : 92.727 %\n",
      "- Recall : 49.515 %\n",
      "- F1 : 0.64557\n",
      "\n",
      "Class unverified Evaluation\n",
      "- Precision : 70.492 %\n",
      "- Recall : 68.254 %\n",
      "- F1 : 0.69355\n",
      "\n",
      "Class false Evaluation\n",
      "- Precision : 88.525 %\n",
      "- Recall : 83.077 %\n",
      "- F1 : 0.85714\n",
      "\n",
      "Combined Evaluation\n",
      "- Accuracy : 83.962 %\n",
      "- Precision : 83.96 %\n",
      "- Recall : 74.039 %\n",
      "- F1 : 0.78688\n",
      "\n",
      "- Average Confidence : 100.0 %\n",
      "Model, Combined,,,,non-rumours,,,true,,,unverified,,,false,,,\n",
      "ReinforcementLearning Test, 83.962, 83.96, 74.039, 0.78688, 84.096, 95.309, 0.89352, 92.727, 49.515, 0.64557, 70.492, 68.254, 0.69355, 88.525, 83.077, 0.85714, \n"
     ]
    }
   ],
   "source": [
    "print(\"Multiclass Classification using Reinforcement Learning\")\n",
    "print(\"\\nValidation Set\")\n",
    "preds = predictions(agent, val_vectors)\n",
    "print(f\"Predictions : {preds.shape}\")\n",
    "\n",
    "# preds = preds.cpu().numpy()\n",
    "print(np.unique(preds))\n",
    "\n",
    "conf_mat = ConfusionMatrix(\n",
    "    labels=np.array([[1 if j == v else 0 for j in range(len(labels_str))] for v in val_labels]),\n",
    "    predictions=np.array([[1 if j == p else 0 for j in range(len(labels_str))] for p in preds]),\n",
    "    binary=False,\n",
    "    model_name=f\"{model_name} Validation\"\n",
    ")\n",
    "conf_mat.evaluate(classes=labels_str)\n",
    "\n",
    "print(\"\\nTest Set\")\n",
    "preds = predictions(agent, test_vectors)\n",
    "print(f\"Predictions : {preds.shape}\")\n",
    "\n",
    "# preds = preds.cpu().numpy()\n",
    "print(np.unique(preds))\n",
    "\n",
    "conf_mat = ConfusionMatrix(\n",
    "    labels=np.array([[1 if j == v else 0 for j in range(len(labels_str))] for v in test_labels]),\n",
    "    predictions=np.array([[1 if j == p else 0 for j in range(len(labels_str))] for p in preds]),\n",
    "    binary=False,\n",
    "    model_name=f\"{model_name} Test\"\n",
    ")\n",
    "conf_mat.evaluate(classes=labels_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df0c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "detected-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "random.seed(33)\n",
    "\n",
    "from library.evaluation import ConfusionMatrix\n",
    "\n",
    "unique_name = \"BERT_Finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loaded-organic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6425, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.loadtxt(\"../../data/processed/vectors/Phemernr2_BERT_base_finetuned_vectors.txt\", delimiter=\",\")\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "skilled-career",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>label2</th>\n",
       "      <th>topic</th>\n",
       "      <th>tvt</th>\n",
       "      <th>cv_fold</th>\n",
       "      <th>tt</th>\n",
       "      <th>tvt2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>552833795142209536</td>\n",
       "      <td>The East London Mosque would like to offer its...</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>charliehebdo-all-rnr-threads</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>580318210609696769</td>\n",
       "      <td>BREAKING - A Germanwings Airbus A320 plane rep...</td>\n",
       "      <td>rumours</td>\n",
       "      <td>true</td>\n",
       "      <td>germanwings-crash-all-rnr-threads</td>\n",
       "      <td>training</td>\n",
       "      <td>3</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552798891994009601</td>\n",
       "      <td>Reports that two of the dead in the #CharlieHe...</td>\n",
       "      <td>rumours</td>\n",
       "      <td>true</td>\n",
       "      <td>charliehebdo-all-rnr-threads</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>576790814942236672</td>\n",
       "      <td>After #Putin disappeared Russian TV no longer ...</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>putinmissing-all-rnr-threads</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>499678822598340608</td>\n",
       "      <td>Saw #Ferguson for myself. #justiceformichaelbr...</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>ferguson-all-rnr-threads</td>\n",
       "      <td>training</td>\n",
       "      <td>3</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                         tweet_text  \\\n",
       "0  552833795142209536  The East London Mosque would like to offer its...   \n",
       "1  580318210609696769  BREAKING - A Germanwings Airbus A320 plane rep...   \n",
       "2  552798891994009601  Reports that two of the dead in the #CharlieHe...   \n",
       "3  576790814942236672  After #Putin disappeared Russian TV no longer ...   \n",
       "4  499678822598340608  Saw #Ferguson for myself. #justiceformichaelbr...   \n",
       "\n",
       "         label       label2                              topic       tvt  \\\n",
       "0  non-rumours  non-rumours       charliehebdo-all-rnr-threads      test   \n",
       "1      rumours         true  germanwings-crash-all-rnr-threads  training   \n",
       "2      rumours         true       charliehebdo-all-rnr-threads      test   \n",
       "3  non-rumours  non-rumours       putinmissing-all-rnr-threads      test   \n",
       "4  non-rumours  non-rumours           ferguson-all-rnr-threads  training   \n",
       "\n",
       "   cv_fold        tt      tvt2  \n",
       "0        2      test  training  \n",
       "1        3  training  training  \n",
       "2        2      test  training  \n",
       "3        2      test  training  \n",
       "4        3  training  training  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../data/processed/phemernr2_dataset_with_tvt.csv\", lineterminator=\"\\n\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d9dc307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-rumours', 'true', 'unverified', 'false']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_str = data['label2'].unique().tolist()\n",
    "labels_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f469a1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 0, 0, 0, 2, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for i, d in data.iterrows():\n",
    "    lab = labels_str.index(d['label2'])\n",
    "#     labels.append([1 if j == lab else 0 for j in range(len(labels_str))])\n",
    "    labels.append(lab)\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adverse-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'training'])\n",
    "val_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'validation'])\n",
    "test_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'testting'])\n",
    "\n",
    "train_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'training'])\n",
    "val_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'validation'])\n",
    "test_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'testting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "demanding-consortium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4299, 768)\n",
      "(1471, 768)\n",
      "(655, 768)\n",
      "(4299,)\n",
      "(1471,)\n",
      "(655,)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape)\n",
    "print(val_vectors.shape)\n",
    "print(test_vectors.shape)\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52df7eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/storage/Work/DataScience/.venv/lib/python3.9/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7253841",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87364793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.ln1 = nn.Linear(n_inputs, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.ln2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.head = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.ln1(x)))\n",
    "        x = F.relu(self.bn2(self.ln2(x)))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58447ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/storage/Work/DataScience/.venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3527a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = 4\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state, label):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e4feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    print(action_batch)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01f5eed9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "2 (<class 'int'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46520/1117496885.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/storage/Work/DataScience/.venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%r (%s) invalid\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 2 (<class 'int'>) invalid"
     ]
    }
   ],
   "source": [
    "num_episodes = train_vectors.shape[0]\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    \n",
    "    # Select and perform an action\n",
    "    state = train_vectors[i_episode]\n",
    "    action = select_action(state, [train_labels[i_episode]])\n",
    "    _, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "    # Observe new state\n",
    "    if not done:\n",
    "        next_state = train_vectors[i_episode + 1]\n",
    "    else:\n",
    "        next_state = None\n",
    "\n",
    "    # Store the transition in memory\n",
    "    memory.push(state, action, next_state, reward)\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "\n",
    "    # Perform one step of the optimization (on the policy network)\n",
    "    optimize_model()\n",
    "    if done:\n",
    "        episode_durations.append(t + 1)\n",
    "        plot_durations()\n",
    "        break\n",
    "\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd07cc1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(\"Multiclass Classification using 4-Layer Linear Network\")\n",
    "# start = time.time()\n",
    "# model_name = f\"Phemernr2_multiclass_4LayerNet_{unique_name}\"\n",
    "# model = NNClassifier(train_vectors.shape[1], n_output=4, criterion=nn.CrossEntropyLoss)\n",
    "# model.train_eval(torch.Tensor(train_vectors),\n",
    "#                 torch.Tensor(train_labels),\n",
    "#                 torch.Tensor(val_vectors),\n",
    "#                 torch.Tensor(val_labels),\n",
    "#                 saves=model_name,\n",
    "#                 n_iter=1000,\n",
    "#                 batch_size=512)\n",
    "# print(f\"Exec Time : {round(time.time() - start, 2)} seconds\")\n",
    "\n",
    "# model.load_pretrained(f\"../../data/models/{model_name}.pth\")\n",
    "\n",
    "# print(\"\\nValidation Set\")\n",
    "# preds = model.predict(val_vectors)\n",
    "# print(f\"Predictions : {preds.shape}\")\n",
    "\n",
    "# preds = preds.cpu().numpy()\n",
    "\n",
    "# conf_mat = ConfusionMatrix(\n",
    "#     labels=np.array([[1 if j == v else 0 for j in range(len(labels_str))] for v in val_labels]),\n",
    "#     predictions=np.array([[1 if j == p else 0 for j in range(len(labels_str))] for p in preds]),\n",
    "#     binary=False,\n",
    "#     model_name=f\"{model_name} Validation\"\n",
    "# )\n",
    "# conf_mat.evaluate(classes=labels_str)\n",
    "\n",
    "# print(\"\\nTest Set\")\n",
    "# preds = model.predict(test_vectors)\n",
    "# print(f\"Predictions : {preds.shape}\")\n",
    "\n",
    "# preds = preds.cpu().numpy()\n",
    "\n",
    "# conf_mat = ConfusionMatrix(\n",
    "#     labels=np.array([[1 if j == v else 0 for j in range(len(labels_str))] for v in test_labels]),\n",
    "#     predictions=np.array([[1 if j == p else 0 for j in range(len(labels_str))] for p in preds]),\n",
    "#     binary=False,\n",
    "#     model_name=f\"{model_name} Test\"\n",
    "# )\n",
    "# conf_mat.evaluate(classes=labels_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a07d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor([[0.345, 0.67, 0.23, 0.45]])\n",
    "x.max(1)[1].view(1, 1)\n",
    "x.gather(1, torch.Tensor([[0.35, 0.87, 0.23, 0.45]]).type(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55461251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e8fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6425, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>tvt2</th>\n",
       "      <th>tvt2_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>552833795142209536</td>\n",
       "      <td>the east london mosque would like to offer its...</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>580318210609696769</td>\n",
       "      <td>breaking - a germanwings airbus a320 plane rep...</td>\n",
       "      <td>true</td>\n",
       "      <td>validation</td>\n",
       "      <td>testting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552798891994009601</td>\n",
       "      <td>reports that two of the dead in the #charliehe...</td>\n",
       "      <td>true</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>576790814942236672</td>\n",
       "      <td>after #putin disappeared russian tv no longer ...</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>validation</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>499678822598340608</td>\n",
       "      <td>saw #ferguson for myself. #justiceformichaelbr...</td>\n",
       "      <td>non-rumours</td>\n",
       "      <td>testting</td>\n",
       "      <td>testting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                         tweet_text  \\\n",
       "0  552833795142209536  the east london mosque would like to offer its...   \n",
       "1  580318210609696769  breaking - a germanwings airbus a320 plane rep...   \n",
       "2  552798891994009601  reports that two of the dead in the #charliehe...   \n",
       "3  576790814942236672  after #putin disappeared russian tv no longer ...   \n",
       "4  499678822598340608  saw #ferguson for myself. #justiceformichaelbr...   \n",
       "\n",
       "         label        tvt2    tvt2_1  \n",
       "0  non-rumours    training  training  \n",
       "1         true  validation  testting  \n",
       "2         true    training  training  \n",
       "3  non-rumours  validation  training  \n",
       "4  non-rumours    testting  testting  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "finetuned_dirname = \"10-epoch-roberta-finetuned-phemernr2-rnr-check-finetune\"\n",
    "\n",
    "data = pd.read_csv(\"../../data/phemernr2_dataset_with_tvt.csv\", sep=\",\")\n",
    "# data = data[['tweet_text', 'tvt2', 'label2']]\n",
    "# data.columns = ['tweet_text', 'tvt2', 'label']\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9160ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['label'].replace(['true', 'unverfied', 'false'], 'rumors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24f98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffeab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\msi_b\\anaconda3\\envs\\pytorch-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class CustomTextDataset(torch.utils.data.dataset.Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels):\n",
    "        self.labels = labels\n",
    "        self.texts = texts\n",
    "        self.attention_mask = None\n",
    "        self.input_ids = None\n",
    "        self.token_type_ids = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"text\": self.texts[idx],\n",
    "            \"label\": self.labels[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx] if self.attention_mask else None,\n",
    "            \"input_ids\": self.input_ids[idx] if self.input_ids else None,\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "    def tokenize(self, tokenizer):\n",
    "        self.attention_mask = []\n",
    "        self.input_ids = []\n",
    "        self.token_type_ids = []\n",
    "\n",
    "        for text in self.texts:\n",
    "            token = tokenizer(text, padding=\"max_length\", truncation=True)\n",
    "            \n",
    "            self.attention_mask.append(token['attention_mask'])\n",
    "            self.input_ids.append(token['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659c5a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 1, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "labels_str = combined_data['label'].unique().tolist()\n",
    "for i, d in combined_data.iterrows():\n",
    "    if d['label'] == \"non-rumours\":\n",
    "        lab = 1\n",
    "    else:\n",
    "        lab = 0\n",
    "    labels.append(lab)\n",
    "    \n",
    "print(len(labels))\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60762c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the east london mosque would like to offer its sincere condolences to the families of those killed during the #charliehebdo attacks (1/2)',\n",
       " 'label': 1,\n",
       " 'attention_mask': None,\n",
       " 'input_ids': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CustomTextDataset(\n",
    "    [d['tweet_text'] for i, d in combined_data.iterrows() if d['tvt2'] == 'training'],\n",
    "    [labels[i] for i, d in combined_data.iterrows() if d['tvt2'] == 'training'])\n",
    "test_dataset = CustomTextDataset(\n",
    "    [d['tweet_text'] for i, d in combined_data.iterrows() if d['tvt2'] == 'validation'],\n",
    "    [labels[i] for i, d in combined_data.iterrows() if d['tvt2'] == 'validation'])\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2c64a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42240030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer([\"you're stuck in a timewrap from 2004 though\", \"summa lumma dumma lumma\"], padding=\"max_length\", truncation=True)\n",
    "# for k,v in inputs.items():\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6ba47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset.tokenize(tokenizer)\n",
    "test_dataset.tokenize(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bdfd4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4336\n",
      "1462\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b4609",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a45758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\",\n",
    "                                                           output_hidden_states=False,\n",
    "                                                           num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28fc013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Steps : 5420\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "save_steps = (round((len(train_dataset)/batch_size) + 0.49)) * epochs\n",
    "# save_steps = 1_000_000\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../../data/models/{finetuned_dirname}\",\n",
    "    num_train_epochs=epochs,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=300,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "print(f\"Save Steps : {save_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e85b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi_b\\AppData\\Local\\Temp\\ipykernel_9372\\3274651076.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "# from copy import deepcopy\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\"] #List of metrics to return\n",
    "#     metric={}\n",
    "#     for met in metrics:\n",
    "#        metric[met] = load_metric(met)\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     metric_res={}\n",
    "#     for met in metrics:\n",
    "#        metric_res[met]=metric[met].compute(predictions=predictions, references=labels)[met]\n",
    "#     return metric_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ed7d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomCallback(TrainerCallback):\n",
    "    \n",
    "#     def __init__(self, trainer) -> None:\n",
    "#         super().__init__()\n",
    "#         self._trainer = trainer\n",
    "    \n",
    "#     def on_epoch_end(self, args, state, control, **kwargs):\n",
    "#         if control.should_evaluate:\n",
    "#             control_copy = deepcopy(control)\n",
    "#             self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "#             return control_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f53d4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2b3df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 542/5420 [02:49<25:01,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4855, 'learning_rate': 9e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 542/5420 [03:06<25:01,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4910690486431122, 'eval_accuracy': 0.8153214774281806, 'eval_runtime': 16.7671, 'eval_samples_per_second': 87.195, 'eval_steps_per_second': 10.914, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1084/5420 [05:54<22:07,  3.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3827, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 20%|██        | 1084/5420 [06:11<22:07,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3831155002117157, 'eval_accuracy': 0.8461012311901505, 'eval_runtime': 16.7657, 'eval_samples_per_second': 87.202, 'eval_steps_per_second': 10.915, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1626/5420 [08:57<19:27,  3.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3078, 'learning_rate': 7e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 30%|███       | 1626/5420 [09:14<19:27,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4376889169216156, 'eval_accuracy': 0.8919288645690835, 'eval_runtime': 16.8274, 'eval_samples_per_second': 86.882, 'eval_steps_per_second': 10.875, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2168/5420 [12:00<16:36,  3.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.256, 'learning_rate': 6e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 2168/5420 [12:17<16:36,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.52899569272995, 'eval_accuracy': 0.884404924760602, 'eval_runtime': 16.7719, 'eval_samples_per_second': 87.17, 'eval_steps_per_second': 10.911, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2710/5420 [15:03<13:48,  3.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1938, 'learning_rate': 5e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 2710/5420 [15:20<13:48,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.64149409532547, 'eval_accuracy': 0.8823529411764706, 'eval_runtime': 16.7758, 'eval_samples_per_second': 87.15, 'eval_steps_per_second': 10.909, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3252/5420 [18:06<11:12,  3.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1322, 'learning_rate': 4.000000000000001e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 3252/5420 [18:23<11:12,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8959925174713135, 'eval_accuracy': 0.8604651162790697, 'eval_runtime': 16.8015, 'eval_samples_per_second': 87.016, 'eval_steps_per_second': 10.892, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3794/5420 [21:10<08:15,  3.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0915, 'learning_rate': 3e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|███████   | 3794/5420 [21:26<08:15,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7785674333572388, 'eval_accuracy': 0.8782489740082079, 'eval_runtime': 16.795, 'eval_samples_per_second': 87.05, 'eval_steps_per_second': 10.896, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4336/5420 [24:13<05:31,  3.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0683, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 4336/5420 [24:30<05:31,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8593408465385437, 'eval_accuracy': 0.8727770177838577, 'eval_runtime': 16.7692, 'eval_samples_per_second': 87.184, 'eval_steps_per_second': 10.913, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4878/5420 [27:16<02:45,  3.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0457, 'learning_rate': 1.0000000000000002e-06, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 4878/5420 [27:33<02:45,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8806429505348206, 'eval_accuracy': 0.8803009575923393, 'eval_runtime': 16.7676, 'eval_samples_per_second': 87.192, 'eval_steps_per_second': 10.914, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5420/5420 [30:20<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0374, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 5420/5420 [30:37<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9133529663085938, 'eval_accuracy': 0.8755129958960328, 'eval_runtime': 16.7655, 'eval_samples_per_second': 87.203, 'eval_steps_per_second': 10.915, 'epoch': 10.0}\n",
      "{'train_runtime': 1837.6678, 'train_samples_per_second': 23.595, 'train_steps_per_second': 2.949, 'train_loss': 0.20009695982141248, 'epoch': 10.0}\n",
      "Execution Time : 1838 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "train = trainer.train()\n",
    "\n",
    "print(f\"Execution Time : {round(time.time() - start)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e70cb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:16<00:00, 10.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9133529663085938,\n",
       " 'eval_accuracy': 0.8755129958960328,\n",
       " 'eval_runtime': 16.7832,\n",
       " 'eval_samples_per_second': 87.111,\n",
       " 'eval_steps_per_second': 10.904,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66faefd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
